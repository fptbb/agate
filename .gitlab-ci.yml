workflow:
  rules:
    - if: $CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS && $CI_PIPELINE_SOURCE == "push"
      when: never
    - if: "$CI_COMMIT_TAG"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: "$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS"
      when: never
    - if: "$CI_COMMIT_BRANCH"
    # Daily Builds
    - if: $CI_PIPELINE_SOURCE == "schedule"

stages:
  - build
  - publish
  - cleanup

build-image:
  stage: build
  tags:
    - saas-linux-2xlarge-amd64
  image:
    name: ghcr.io/blue-build/cli
    entrypoint: [""]
  services:
    - docker:dind
  parallel:
    matrix:
      - RECIPE:
          - recipe.yml
  variables:
    # Setup a secure connection with docker-in-docker service
    # https://docs.gitlab.com/ee/ci/docker/using_docker_build.html
    DOCKER_HOST: tcp://docker:2376
    DOCKER_TLS_CERTDIR: /certs
    DOCKER_TLS_VERIFY: 1
    DOCKER_CERT_PATH: $DOCKER_TLS_CERTDIR/client
  before_script:
    # Pulls secure files into the build
    - curl --silent "https://gitlab.com/gitlab-org/incubation-engineering/mobile-devops/download-secure-files/-/raw/main/installer" | bash
    - export COSIGN_PRIVATE_KEY=$(cat .secure_files/cosign.key)
    - docker login $BB_REGISTRY -u $BB_USERNAME -p $BB_PASSWORD
  script:
    - sleep 5 # Wait a bit for the docker-in-docker service to start
    - echo "Pushing to $BB_REGISTRY/$BB_REGISTRY_NAMESPACE/agate"
    - bluebuild build --verbose --cache-layers --compression-format zstd --push ./recipes/$RECIPE

publish-metadata:
  stage: publish
  image: alpine:latest
  variables:
    REGCTL_VERSION: v0.5.0
    # The target tag specifically for Artifact Hub verification
    DEST_REPO: "quay.io/fptbb/agate:artifacthub.io"
  rules:
    # Only run on the default branch (usually main/master) or tags
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_COMMIT_TAG
  before_script:
    # 1. Install regctl dependencies and binary
    - apk add --no-cache curl
    - curl -L https://github.com/regclient/regclient/releases/download/${REGCTL_VERSION}/regctl-linux-amd64 > /usr/local/bin/regctl
    - chmod +x /usr/local/bin/regctl
    
    # 2. Authenticate with Quay.io using your existing bot credentials
    - echo "$BB_PASSWORD" | regctl registry login quay.io -u "$BB_USERNAME" --pass-stdin
  script:
    # 3. Verify the config file exists before trying to push
    - |
      if [ ! -f "artifacthub-repo.yml" ]; then
        echo "Error: artifacthub-repo.yml not found in the root of the repository."
        exit 1
      fi
      
    # 4. Push the metadata artifact
    - |
      regctl artifact put \
        --format '{{ .Manifest.GetDescriptor.Digest }}' \
        --artifact-type application/vnd.cncf.artifacthub.config.v1+yaml \
        -f artifacthub-repo.yml \
        --file-media-type "application/vnd.cncf.artifacthub.repository-metadata.layer.v1.yaml" \
        "$DEST_REPO"

cleanup-old-tags:
  stage: cleanup
  image: python:3-alpine
  variables:
    IMAGE_NAME: agate
    MAX_AGE_DAYS: "7"
    MAX_KEEP: "5"
    DRY_RUN: "false"
  script:
    - pip install requests
    - |
      python3 -c '
      import os
      import sys
      import json
      import logging
      import requests
      from datetime import datetime, timezone
      from requests.auth import HTTPBasicAuth

      logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
      logger = logging.getLogger(__name__)

      class DockerV2Cleaner:
          def __init__(self):
              self.registry = "quay.io"
              namespace = os.environ.get("BB_REGISTRY_NAMESPACE")
              image = os.environ.get("IMAGE_NAME")
              self.repo = f"{namespace}/{image}"
              self.username = os.environ.get("BB_USERNAME")
              self.password = os.environ.get("BB_PASSWORD")
              self.dry_run = os.environ.get("DRY_RUN", "false").lower() == "true"
              
              if not all([namespace, image, self.username, self.password]):
                  logger.error("Missing variables.")
                  sys.exit(1)

              self.session = requests.Session()
              self.base_url = f"https://{self.registry}/v2"
              self.token = self.get_auth_token()
              self.session.headers.update({
                  "Authorization": f"Bearer {self.token}",
                  "Accept": "application/vnd.docker.distribution.manifest.v2+json, application/vnd.oci.image.manifest.v1+json"
              })

          def get_auth_token(self):
              service = "quay.io"
              scope = f"repository:{self.repo}:pull,push"
              auth_url = f"https://{self.registry}/v2/auth?service={service}&scope={scope}"
              try:
                  resp = requests.get(auth_url, auth=HTTPBasicAuth(self.username, self.password))
                  if resp.status_code != 200:
                      logger.error(f"Auth Failed: {resp.text}")
                      sys.exit(1)
                  return resp.json().get("token")
              except Exception as e:
                  logger.error(f"Auth Error: {e}")
                  sys.exit(1)

          def get_all_tags(self):
              url = f"{self.base_url}/{self.repo}/tags/list"
              all_tags = []
              while url:
                  try:
                      resp = self.session.get(url)
                      if resp.status_code != 200: break
                      data = resp.json()
                      all_tags.extend(data.get("tags", []))
                      link = resp.headers.get("Link")
                      url = None
                      if link and "rel=\"next\"" in link:
                          url = link.split(";")[0].strip("<>")
                          if not url.startswith("http"): url = f"https://{self.registry}{url}"
                  except: break
              return all_tags

          def parse_date(self, date_str):
              try:
                  date_str = date_str.replace("Z", "+00:00")
                  if "." in date_str and "+" in date_str:
                      main, tz = date_str.split("+")
                      date_str = f"{main[:26]}+{tz}"
                  return datetime.fromisoformat(date_str)
              except: return None

          def get_tag_metadata(self, tag):
              try:
                  man_resp = self.session.get(f"{self.base_url}/{self.repo}/manifests/{tag}")
                  if man_resp.status_code != 200: return None
                  
                  manifest = man_resp.json()
                  digest = man_resp.headers.get("Docker-Content-Digest")
                  dt = None

                  # Strategy A: OCI Annotations (Common in .sig)
                  if "annotations" in manifest:
                      created = manifest["annotations"].get("org.opencontainers.image.created")
                      if created: dt = self.parse_date(created)

                  # Strategy B: Config Blob (Standard Images)
                  if not dt and "config" in manifest:
                      config_digest = manifest["config"].get("digest")
                      if config_digest:
                          blob_resp = self.session.get(f"{self.base_url}/{self.repo}/blobs/{config_digest}")
                          if blob_resp.status_code == 200:
                              dt = self.parse_date(blob_resp.json().get("created"))

                  # Strategy C: Legacy History
                  if not dt and "history" in manifest and len(manifest["history"]) > 0:
                      try:
                          v1 = json.loads(manifest["history"][0]["v1Compatibility"])
                          dt = self.parse_date(v1.get("created"))
                      except: pass

                  if dt:
                      if dt.tzinfo is None: dt = dt.replace(tzinfo=timezone.utc)
                      return {"name": tag, "date": dt, "digest": digest}
                  
                  logger.warning(f"No date found for {tag}, skipping safety check.")
                  return None
              except: return None

          def delete_tag(self, tag, digest):
              if self.dry_run:
                  logger.info(f"[DRY RUN] Delete {tag}")
                  return True
              resp = self.session.delete(f"{self.base_url}/{self.repo}/manifests/{digest}")
              if resp.status_code in [200, 202, 204]:
                  logger.info(f"Deleted {tag}")
                  return True
              logger.error(f"Failed delete {tag}: {resp.status_code}")
              return False

          def is_cosign_sig(self, tag_name):
              """Checks if tag looks like sha256-....sig"""
              return tag_name.startswith("sha256-") and tag_name.endswith(".sig")

          def get_cosign_sig_name(self, image_digest):
              """Calculates the expected .sig tag name for a given image digest"""
              # sha256:abcdef... -> sha256-abcdef....sig
              clean_digest = image_digest.replace(":", "-")
              return f"{clean_digest}.sig"

          def run(self):
              max_age_days = int(os.environ.get("MAX_AGE_DAYS", 7))
              max_keep = int(os.environ.get("MAX_KEEP", 5))
              protected_tags = ["latest", "latest-cache"]

              logger.info(f"Scanning {self.repo}...")
              tags = self.get_all_tags()
              logger.info(f"Found {len(tags)} tags. Fetching metadata...")

              tag_data = []
              for tag in tags:
                  meta = self.get_tag_metadata(tag)
                  if meta: tag_data.append(meta)

              # Sort newest first
              tag_data.sort(key=lambda x: x["date"], reverse=True)

              # === PASS 1: Identify Protections ===
              kept_digests = set()
              kept_sig_names = set()
              
              image_count = 0
              
              for item in tag_data:
                  name = item["name"]
                  digest = item["digest"]
                  
                  # Explicit Protection (latest, etc)
                  if name in protected_tags:
                      kept_digests.add(digest)
                      kept_sig_names.add(self.get_cosign_sig_name(digest))
                      continue

                  # Skip signatures during the "Count" phase (we only count real images against MAX_KEEP)
                  if self.is_cosign_sig(name):
                      continue

                  # Count Check
                  if image_count < max_keep:
                      kept_digests.add(digest)
                      kept_sig_names.add(self.get_cosign_sig_name(digest)) # Protect its signature!
                      image_count += 1

              logger.info(f"Identified {len(kept_digests)} images to keep (and their signatures).")

              # === PASS 2: Execute Cleanup ===
              for item in tag_data:
                  name = item["name"]
                  digest = item["digest"]
                  age_days = (datetime.now(timezone.utc) - item["date"]).days

                  # 1. Is this an explicitly kept image?
                  if name in protected_tags:
                      logger.info(f"Keeping {name} (Protected Name)")
                      continue
                  
                  # 2. Is this an image we decided to keep?
                  if digest in kept_digests and not self.is_cosign_sig(name):
                      logger.info(f"Keeping {name} (Within Keep Limit)")
                      continue

                  # 3. Is this a Signature for a kept image?
                  if name in kept_sig_names:
                      logger.info(f"Keeping {name} (Signature for active image)")
                      continue

                  # 4. Age Check
                  if age_days > max_age_days:
                      self.delete_tag(name, digest)
                  else:
                      logger.info(f"Keeping {name} (Age: {age_days}d)")

      if __name__ == "__main__":
          cleaner = DockerV2Cleaner()
          cleaner.run()
      '